<!DOCTYPE html>
<html>

  <head>
    <script src="https://docs.opencv.org/master/opencv.js" type="text/javascript"></script>
    <script src="https://docs.opencv.org/master/utils.js" type="text/javascript"></script>

    <script type='text/javascript'>
      cv['onRuntimeInitialized'] = initDetection;


function initDetection() {
  const captureHeight = 480;
  const captureWidth = 640;

  const outputElement = document.getElementById('output');
  const cameraElement = document.createElement('video');


  let frameBuffer = new cv.Mat(captureHeight, captureWidth, cv.CV_8UC4);


  downloadModels()
    .then(createProcessingStep)
    .then(
      processingStep => ({
        cameraStream: setupCameraStream(cameraElement, captureHeight, captureWidth),
        processingStep: processingStep
      })
    ).then(
      objs => {
        console.dir(objs.cameraStream);
        return captureFrame(objs.cameraStream, frameBuffer, outputElement, objs.processingStep);
      }
    ).then(
      () => console.log('capturing started')
    );

}


function downloadModels() {  
  //console.log('download models started');

  const faceDetectionPaths = {
    proto: "opencv_face_detector.prototxt",
    caffe: "opencv_face_detector.caffemodel"
  };

  const utils = new Utils('');

  const downloadFaceDetectionPromise = Promise.all([
    new Promise( (resolve, reject) =>
      utils.createFileFromUrl(faceDetectionPaths.proto, faceDetectionPaths.proto, resolve)
    ),
    new Promise( (resolve, reject) =>
      utils.createFileFromUrl(faceDetectionPaths.caffe, faceDetectionPaths.caffe, resolve)
    )
  ])
    .then(
      () => cv.readNet(faceDetectionPaths.proto, faceDetectionPaths.caffe)
    );

  return Promise.all([downloadFaceDetectionPromise])
    .then(nets => ({
      faceDetection: nets[0]
    })
    );
}


function captureFrame(cameraStream, frameBuffer, outputElement, processingStep) {
  //console.log('captureFrame called');
  cameraStream.read(frameBuffer);
  const processedFrame = processingStep(frameBuffer);
  cv.imshow(outputElement, processedFrame); 
  processedFrame.delete();

  const frameTimeout = 0;
  // Repeat every 50ms
  setTimeout(() => captureFrame(cameraStream, frameBuffer, outputElement, processingStep), frameTimeout);
};



function setupCameraStream(cameraElement, height, width) {
  cameraElement.setAttribute('width', width);
  cameraElement.setAttribute('height', height);

  // Get a permission from user to use a cameraElement.
  navigator.mediaDevices.getUserMedia({video: true, audio: false})
    .then((stream) => {
      cameraElement.srcObject = stream;
      cameraElement.onloadedmetadata = e => cameraElement.play();
    });

  // return open cameraElement stream
  return new cv.VideoCapture(cameraElement);
}


function createProcessingStep(models) {
  //console.log('Creating processing step');
  return function(frame) {
    //console.log('Running processing step');

    const processedFrame = frame.clone();
    cv.cvtColor(frame, processedFrame, cv.COLOR_RGBA2BGR);
    const blob = cv.blobFromImage(processedFrame, 1, {width: 192, height: 144}, [104, 117, 123, 0]);

    const net = models.faceDetection;
    net.setInput(blob);
    const out = net.forward();
    for (let i = 0, n = out.data32F.length; i < n; i += 7) {
      const confidence = out.data32F[i + 2];
      if (confidence < 0.5) { continue; }
      const left = out.data32F[i + 3] * frame.cols;
      const top = out.data32F[i + 4] * frame.rows;
      const right = out.data32F[i + 5] * frame.cols;
      const bottom = out.data32F[i + 6] * frame.rows;
      cv.rectangle(processedFrame, {x: left, y: top}, {x: right, y: bottom}, [0, 255, 0, 255]);
    }
    blob.delete();
    out.delete();

    cv.cvtColor(processedFrame, processedFrame, cv.COLOR_BGR2RGBA);
    return processedFrame;
  };
}

    </script>

  </head>

  <body>
    <canvas id="output" width=640 height=480 style="max-width: 100%"></canvas>
  </body>

</html>
